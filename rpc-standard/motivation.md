

Networking is not very good or easy right now and no project I've been able to find could actually fix it. I've tried to get involved with several projects that get maybe a third of the problems solved to propose incremental changes that bring it closer to the remaining two thirds, and been repeatedly ignored and turned down. Instead, I'll start from scratch and lay out what the problems are and what components any attempted solution must have. Hopefully that gets some attention and someone will either be inspired to fix their project or join mine.

To briefly mention several problems, there's head of line blocking, lack of schematization with migratability and translatability and a useful type system, poor compression, poor security, and poor language support. These are related problems, and solving one will frequently make others easier.


## Security

Security wise, capability security is the gold standard for security; it's the most secure way to do things, and the most intuitive to use and easiest to analyze for static security. Fortunately, it's also pretty easy to implement and scales very well to novel APIs, with a set of well researched constructions that can replicate the function of any other security system. Other people have already written a lot about this, so I won't go too in depth. If people want to know more, I'll post a followup about my own novel capsec throughts and/or link to prior art.

## Promise Pipelining

Promise pipelining is one of the core features that makes all the other latency reduction techniques work; promise pipelining is a technology that allows you to refer to the result of a request before it returns as part of the parameters of another request. With promise pipelining, every request you send is given an ID as you send it that the other side of the connection can understand, and requests can refer to that ID as part of their payloads. To be practical, this requires the network system come with some kind of object model to be able to express what it means to refer to the response of a request, because sending over parsing logic to extract things from a serialized representation is... problematic. Whatever language is available to express how to do this *must* be sub-turing-complete so that it isn't possible to do long running or hostile tasks with it, it must be an extremely compact serialized bytecode that can get lost in the corner of a single UDP packet for simple operations, it must be very carefully locked down to avoid security problems by construction (processor vendors, please stop selling us broken hardware, it would make my job a lot easier if I could formally verify security against (a formalized version of) what you say your hardware does and just verify the hardware does as specified instead of needing to invent my own much messier formalism capturing a superset of what it actually does and hoping I'm correct. Please.), it must *not* have a common source code representation that people are expected to use but instead be synthesized directly by the networking library from promise manipulations done by the host language, it must be easy to generate so that it's practical to demand every language library implement a synthesizer, it must be able to be turned off cleanly without affecting the functionality or security of the protocol (only the performance, just in case processor vendors make another catastrophic fucky wucky and then don't fix it for years. again.), it must be impossible to make any part of the checking or evaluation of the reference take longer than linear time, and it must be possible to partially evaluate when some of the pending requests complete. Some of that merits expansion, so in no particular order:

Promise pipelining is a critical performance and usability feature for general purpose network protocols, because promise pipelining is what enables the single responsibility principle. Round trips are really really expensive compared to compute, and usually even compared to bandwidth. I encourage you to try to calculate what the break-even point is for sending, say 20% more data (a generous estimate) versus taking one additional full round trip is with respect to application responsiveness for your favorite server and device. How much data do you need to be sending constantly to saturate the connection enough that you'd rather do the latter than the former. And then consider that promise pipelining will usually *save* you data by allowing you to entirely avoid transmitting the intermediate results both ways. Without promise pipelining, to realize this performance benefit it is necessary to contort the API design around it, rather than having a single method/endpoint for each logical operation, making a single endpoint for each chain of operations that's commonly used, so instead of "search for thing" then "display details of thing" you'd get a single "search and display details" for each combination of ways things can be searched or displayed, producing a combinatorial explosion of handling, especially once you start chaining more things, for example "show the details of the thing's friend" or "count how many friends the thing has". This lack of compositionality means that backend servers frequently have to add new endpoints for every single new feature the frontend wants rather than just providing the set of operations the frontend may perform and letting the frontend compose them as appropriate. GraphQL is an example of this kind of aggregation in more conventional web technologies, where the backend can just express the set of operations available to the frontend and the common infrastructure provides a framework for composing them without extra round trips. GraphQL is of course very wasteful, using JSON and textual inputs for everything, and has a very sharp client/server distinction which limits the usefulness for more flexible architectures.

Regarding the linearity of checking and evaluation, we could instead have a VM that counts instruction costs to either limit them or charge them to an account in a variety of ways, for example by requiring the other side of the connection to present partial hash inversions to prove they spent at least as much effort preparing to ask you to run the code as they're expecting you to expend on their behalf, or by requiring every user to sign in to an accounting system that can charge tiny fractions of money for every request sent, etc. However, it is my opinion that requiring global internet finance integration for every single service that runs the infrastructure would be awful for unbanked, marginalized populations, and hobbyists who just want to play with small services, and therefore we shouldn't do it. Proof of work is also wasteful, and we should look harder for alternatives rather than baking that into more parts of our networking primitives; as anubis has shown, it might be valuable to require some proof of work as an anti-hostile-crawler/ddos measure, but I don't really want to get into that right now. More importantly, gas based VMs with more complicated control flow means increased vulnerability to side channel attacks and ability to run attack code and other hazards, so restricting to linear runtime servers dual purposes of both reducing attack surface and requiring the originator to have paid for the network transmission for every instruction they want you to execute. Calling methods can then be metered or billed if and as the host wants at the method level. Proving resistence to specter in a general VM is really hard actually, requiring things like working out exactly what it takes to separate a bounds check from an access well enough to ensure that cache misses, pipeline stalls, instruction reordering, and speculation can't bring them back together enough to form a complete exploit mechanism. Designing a mechanism that has no ability to make a scratch array, no ability to branch backwards to loop, no ability to write at variable offsets, gets a different randomly positioned stack space every time it is evaluated, and costs the attacker more money than the reciever makes it a lot easier. Since we live in a world where specter exists, and hasn't been fixed after years but only partially mitigated, along side vulnerabilities in constant time cryptography instructions, and so many others, making the mechanism complicated to secure is making it broken and insecure.

One of the possible objections to such a restricted mechanism is the way it prevents running things like filter/map/reduce queries locally with the data it is querying over, but while that is a valuable use case, this specific mechanism is a poor fit for it. Promise pipelining like this is inherently an imperative kind of action, "run this method, then run this method, then run this method" which means that if it is being used to perform queries directly, then it is forced to do full sequential loops across the whole dataset, or requires it to have been given access to all the relevant indices as object references and manually plan the query with respect to them which complicates the security dramatically. Or the promise pipelining VM language could have added features until it includes an entire subsystem for expressing relational queries and whatever other kinds of queries people want to use, but remember that the goal is to minimize the attack surface and runtime cost of a security and performance critical subsystem rather than multiply it exponentially. We can implement a querying subsystem separately anyways without baking it into the protocol, as a query object that can be referenced and requested and secured however is desired, that has a method that takes query descriptions and can plan and execute the queries with the more complicated query semantics, and can use the promise pipelining system to chain inputs and outputs between queries and other requests in a uniform manner, which allows the query system to evolve separately from the base protocol. Perhaps a successor protocol designed when we have much better computer science knowledge regarding compositional formalisms of queries and databases than we do now, cheaper computation than we do now, and better security and secure hardware than we do now could revisit this decision.

A possible pitfall of designing and implementing a promise pipelining system badly is making it distinguish between "pointers" and "primitives" so that the promise pipelining system can only operate on "pointers" by doing something like making references to previous requests or objects be implemented by something that's either a reference or a pointer indirection, essentially creating a linked list of much more expensive pointer offset ops rather than using a more compact unified model of message patching. Don't do that. Doing that forces interface awkwardness every time pipelining on a primitive is desired, requiring making every primitive in any interface that could want to have a primitive pipelined able to be enum between the primitive and an interface that has a single method to retrieve the primitive. This adds a ton of noise to using the interface since it now needs a bunch of enum wrappers, breaks separation of concerns by requiring the interface authors to consider and write workarounds for ways the interface can be pipelined, and inflates the size of the messages on the wire with all the additional enum tags and pointer references. Just choose not to make the obvious mistake.

## Sequencing and Unsequencing

Head of line blocking has been partially solved a couple times but people keep inventing it again and again every time, and I'm getting really frustrated with it. Head of line blocking can be regarded as spurious causal dependencies. Head of line blocking is a symptom of stream oriented systems (though not necessarily exclusive to them), where everything has a fully linear global order, and if one thing is slower everything "behind" it gets slowed down, even if there's no reason for it to depend on the previous packet. This can occur at the packet level, the content level, the request level, the application level, etc. in a bunch of ways. At the lowest level, when talking over a stream based protocol like TCP, if one packet gets dropped by congestion or intermittent link failure or anything, any subsequent packets have to wait for it to be retransmitted before they can be delivered. As an example of this, if you, for example, stream a video over a stream, and part way through realize that the network doesn't support the video's bitrate in realtime, if you send a request for a lower bitrate version the server can't start sending a new version of the video until you've recieved everything it "already" sent even if it never arrived, at a much lower rate than realtime forcing you to wait while a quality of video you don't want to watch anymore stuttery and slowly fills in before you start seeing the quality you can actually watch. To bypass this, HTTP based video serving systems spent a whole bunch of effort inventing a multiplexing protocol on top of the stream that establishes multiple streams and then sends different parts along different streams so that when they request a new lower quality version streams that are delivering the high quality version can be immediately killed without blocking the new version, etc. This is a complicated and inefficient solution that has spent many engineering hours adapting a technology never meant for this to support it.

Consider also a progressive jpg: A progressive jpg (drastically simplifying, conflating features of a couple different versions (and maybe some speculative future tech)) consists of a header that gives a description of the shape of the image and some common information about how it should be decoded. Then the image is broken into a series of regions, ordered from larger to smaller, starting with the whole image at low detail, then splitting it into smaller regions with higher detail. During decoding, a higher detail region is layered on top of the lower detail region it refines (potentially adding to it to reduce the amount of baseline offset needed to encode in the child region and improving compression ratios). Therefore there is no causal dependency between sibling regions, only between parent and child regions. Transmitting this in a stream creates head of line blocking, where the reciever can't start decoding the top left region of the next layer until the bottom right region of the previous layer is fully recieved, and if any single packet is dropped the entire image stops refining until a whole retransmission round trip happens, giving a worse experience and higher latencies. If the underlying connection instead provided a ... Causal DAG .. of some kind where the jpeg encoding knowledge could just mark the exact dependencies between regions and the reciever could start decoding without waiting for siblings to complete, this would be a significant improvement for even ordinary web browsing, but beyond that something with nanite-style virtualized geometry and textures that supports any user generated content that needs to be streamed would be utterly unusable without a system like this. Expressing these dependencies is relatively straightforward in the paradigm of Object Capability based protocols with promise pipelining, but nothing I'm aware of even tries to do this despite being a huge performance win, vitally important for future technological development, and really naturally coupled to the causal requirements of ocap systems. There is a fair bit of work to do here figuring out how to compact the causal dag into as few bits as possible for common usecases and make programming language libraries naturally and intuitively express the causality. There's a bit of prior art in programming language causality usability like ponylang, and I have a plan for how to implement this all myself, but I'd really like to see other people researching, experimenting, and playing with it since more eyes makes the odds of finding something I missed higher.

Head of line blocking also has a mirror problem from the lack of being able to state that one request should follow another. In the classic HTTP paradigm, it's expected to make one request per stream connection, which means that if you want to POST one thing that affects another endpoint then GET the second endpoint to see the final state, you can't just send both requests in one stream and rely on the server not processing the second stream until the first completes. You instead have to send the POST request, wait for the response, then send the GET request. This doubles the latency over being able to specify causal ordering of requests so that requests can be pipelined before all their dependencies are satisfied. Additionally the single-stream-multi-connection architecture encourages DDOSing your own servers by making the intended usage of the service involve every user making dozens of connections simultaneously opening dozens of file handles requiring the kernel to allocate dozens of buffers which largely store redundant copies of the client identifying itself to the server. It should be easier to tell your own software and DDOS/slow loris attacks apart!

The model I'm proposing can be built on top of UDP, be fully encrypted with 0rtt initialization, and supports all the above usecases with low overhead. We can specify a causal dag, where each message can specify a set of dependencies, which it may pull data from or just have a causal dependency on. The sender of a message may refer to any message it wants to send as soon as it picks a message ID for it. The receiver of a message will wait until the dependencies of a message are satisfied to process it. This allows pipelining arbitrarily many messages in sequence and running arbitrary graphs of messages in optimal parallelism, while minimizing overhead. The exact layout of the format to take as few bits as possible on a message, and what all a message can be, can be discussed later.

We can define the semantics of the model as a directed acyclic graph of dependencies. Without getting too deep into how these properties are established, Each machine involved in a network has a local view of the network causality with regards to the DAG of messages, and no machine will ever observe the causality being violated. If two machines are both implementing the protocol correctly, then the pair of them together will never observe causality being violated even if they are both talking to a malicious actor and making causality constraints with respect to the third party; with a cryptographic integrity assumption this holds even if the malicious party is allowed to relay, reorder, drop, and duplicate packets. (at least up to "the connection will drop safely before any causality violating message is processed by the application" so an attacker can't force a semantic violation only strategically permit valid progress and cause a total failure in a manner the application can detect and react to.) The semantics are defined by "happens before" edges between messages, and can be expressed whether or not the messages have been sent yet. The "real" order can be defined as any linearization which preserves the DAG, which permits parallel processing, and the elision of constraints that it's no longer possible to violate to reduce packet sizes.

Of course we must assume that it is possible for a user to obtain a non-compromised computer with un-backdoored cryptographic primitives in order to progress with those security properties; justifying and establishing these assumptions is left as an exercise to the reader, sorry.


## Reliability and Unreliability

In the previous section about head of line blocking, we've talked about Sequencing and Unsequencing network requests. In classic networking protocols, you must pick between these; TCP gives you sequenced stream and UDP gives you unsequenced datagrams. But there's a second dimension here; TCP is a *reliable* sequenced stream and UDP provides *unreliable* datagrams. There are two more corners of that matrix which are necessary for a practical protocol, and I intend to not only cover those corners and why they're necessary, but provide a unifying model for everything in between like I did in the previous section.

In addition to reliable sequencing, there can be unreliable sequencing, where an earlier packet may never arrive after a later packet, but rather than the later packet being held until all earlier packets arrive, the later packet is delivered immediately and any earlier packets are then dropped when they arrive. This ensures that the latest up to date information is delivered as soon as it becomes available, and the application doesn't need to worry about recieving updates out of order. This is an excellent fit for things like progress notifications, where we don't care about hearing that it reached 68% complete if we already heard that it reached 70% complete. It can also fit very well for video game physics updates, since once we know the positions and velocities (and hitpoints and status effects) of everything in the scene, the client doesn't need the previous version anymore and can instead interpolate directly to the latest version, reducing lag even when under network load. Voice calls may also be suitable for this; dropping out of order sound chunks instead of playing them out of order may give a better experience, though tuning it to see how large the chunks should be and how much latency is beneficial to introduce would be necessary.

Reliable unsequencing can express the example of the progressive jpeg from earlier, where we don't care in which order the regions arrive as long as they all do. The additional power of the sequencing edges that ensure child regions arrive after parent edges from the model I proposed earlier, however, can slightly simplify the code to handle the image, and also improves the performance by allowing the retransmission logic to know which packets are most important to retransmit first for the best UX, while providing a good abstraction for providing that information at the application layer without needing to manually tear down the abstraction stack to mess with packets directly. Reliable unsequencing in general is the most efficient way to download files, which is why bittorrent implements it in a slightly hacky way, allowing the chunks of the file or collection of files to arrive in any order.

As we can see, all four quadrants of the matrix are vitally important for some purposes, and most applications (a chat program that can send both text messages and images, web browsers that load pages that have assets and call server endpoints, video games that can load assets while sharing physics updates and having in game voice chat) benefit from using multiple quandrants, including the nonstandard two I just described, and indeed would be most naturally expressed by switching back and forth between them within the scope of a single connection, also known as quadrant vaccilation. (That's a joke, it isn't actually called that, I just included that to jumpscare people with a specific internet background) The `enet` library for networking in games successfully implements all four quadrants on top of a UDP carrier, but the fixed channel layout and lack of encryption makes it unsuitable for more general usage.

However, you may have noticed a slight missing case in that previous model; in the unreliable sequenced case, it's frequently useful to retransmit the latest state to ensure the latest state arrives eventually if it isn't superceeded. For example in the progress notifications model, you do actually want to hear the very latest notification eventually even if it were lost initially and needed to be retransmitted. This is especially clear on the 100% state; it's desirable to actually know the process is finished. But even when there isn't a specific known stopping point it's still useful to learn the latest state eventually, even when future updates are still coming. For example, in any kind of collaborative editing system like r/place (to sidestep the complexities of crdts for a more flexible data layout, which still works but would require a detour into the antimatter history pruning crdt algorithms to explain how to make losing intermediate updates safe in a distributed system so people with moderate knowledge of the state of the art in document synchronization don't get mad) losing intermediate states that were never reflected on the client is fine, but as a user it would be highly preferable if a pixel didn't stay the wrong color on your screen until someone else drew on it.

To express these semantics, we can add the concept of a "supercession" relation (or rather variant of the happens before relation) into the causal DAG. This expands the set of possible linearizations to include those where a subset of messages which are only transitively accessible via supercession edges from messages which are included are omitted. It continues to exclude any linearization where a superceeded message comes after the successor.

### Fault Tolerance

When handling reliable or semi-reliable transport, it is important to address the fault tolerance properties. As we know from the byzantine fault tolerance theorems, it's not possible to get perfect notification on both sides in all cases, so instead we should be clear about exactly where the fault tolerance guarantee lies. For these purposes, the fault tolerance guarantee is that no sequenced operation will happen while one of its prerequisites didn't regardless of the network conditions and that on a graceful termination, the sender will know that everything it sent that is reliably constrained to happen before the end of the connection will have happened, though it isn't possible to know that the other side of the connection also knows that everything the other side sent also arrived. Heartbeat packets however can maintain the regress of "I know that you know that I know" while the connection is otherwise idle, and the causal constraints will establish that earlier messages are mutually shared, though most applications won't care about the exact level of assurance on each message and instead just rely on the causal coherence and safe termination to get it good enough for their purposes.

Transactions would be a useful enhancement as well, to guarantee that of a batch of messages, the state and causality reflect either all of the messages or none of the messages, even in the presence of connection failures. For single party transactions it's relatively straightforward, just adding a mechanism to the connection to create a transaction, then attach messages to the transaction, then complete the transaction, which could have no overhead when not in use because it can be fit inside an existing enum field that's already required in the protocol. Multiparty transactions, however, which could integrate multiple peers in a transaction, allowing callbacks to another machine and other multi-party operations, is much more complicated and it's not clear to me how to implement these in an easily usable mechanism which can be widely adopted across multiple interacting systems. I know it's possible to implement this kind of technique because existing distributed database engines have this feature, but the documentation doesn't go into detail into how to implement them. It would probably need significant runtime support to make it low effort on application writers. If anyone happens to have the required specialist knowledge to talk about how to build this sort of thing, please get in touch, I'd love to work together.

## Multiparty features

If networking were always only between two peers, it would be pointless because you could just pick the two peers up and bring them to the same room. To actually be valuable, networking must have support for connecting to multiple peers. In fact, without any multiparty features the only option to establish networking is to have gone to the same room, and run cables. Therefore, the ability to work with multiparty features is a major component of networking protocols being usable.

One of the fundamental pieces of modern multiparty support is URL/URI/IRI features, which creates an international standard for encoding in text an unambiguous, machine-parseable instruction to connect to a specific peer and asking it for a specific thing. It has a few shortcomings of course, but it's such a good standard, so well architected, and so widely useful that it is mandatory for basically everything going forward, and if it ever dies it will be after a dramatic change to the way that data and documents are stored in general and then many decades of everything on the internet slowly being ported to the new formats until the last printed QR code containing a URL is destroyed. I don't actually think it's meaningfully possible to improve the standard for that at this point, it's just so well optimized for its purpose and medium. That said, what don't they capture well? They don't embed the cryptographic certificates for 0rtt connection establishment, they can't do promise pipelining across them