

Networking is not very good or easy right now and no project I've been able to find could actually fix it. I've tried to get involved with several projects that get maybe a third of the problems solved to propose incremental changes that bring it closer to the remaining two thirds, and been repeatedly ignored and turned down. Instead, I'll start from scratch and lay out what the problems are and what components any attempted solution must have. Hopefully that gets some attention and someone will either be inspired to fix their project or join mine.

To briefly mention several problems, there's head of line blocking, lack of schematization with migratability and translatability and a useful type system, poor compression, poor security, and poor language support. These are related problems, and solving one will frequently make others easier.


## Security

Security wise, capability security is the gold standard for security; it's the most secure way to do things, and the most intuitive to use and easiest to analyze for static security. Fortunately, it's also pretty easy to implement and scales very well to novel APIs, with a set of well researched constructions that can replicate the function of any other security system. Other people have already written a lot about this, so I won't go too in depth. If people want to know more, I'll post a followup about my own novel capsec throughts and/or link to prior art.

## Sequencing and Unsequencing

Head of line blocking has been partially solved a couple times but people keep inventing it again and again every time, and I'm getting really frustrated with it. Head of line blocking can be regarded as spurious causal dependencies. Head of line blocking is a symptom of stream oriented systems (though not necessarily exclusive to them), where everything has a fully linear global order, and if one thing is slower everything "behind" it gets slowed down, even if there's no reason for it to depend on the previous packet. This can occur at the packet level, the content level, the request level, the application level, etc. in a bunch of ways. At the lowest level, when talking over a stream based protocol like TCP, if one packet gets dropped by congestion or intermittent link failure or anything, any subsequent packets have to wait for it to be retransmitted before they can be delivered. As an example of this, if you, for example, stream a video over a stream, and part way through realize that the network doesn't support the video's bitrate in realtime, if you send a request for a lower bitrate version the server can't start sending a new version of the video until you've recieved everything it "already" sent even if it never arrived, at a much lower rate than realtime forcing you to wait while a quality of video you don't want to watch anymore stuttery and slowly fills in before you start seeing the quality you can actually watch. To bypass this, HTTP based video serving systems spent a whole bunch of effort inventing a multiplexing protocol on top of the stream that establishes multiple streams and then sends different parts along different streams so that when they request a new lower quality version streams that are delivering the high quality version can be immediately killed without blocking the new version, etc. This is a complicated and inefficient solution that has spent many engineering hours adapting a technology never meant for this to support it.

Consider also a progressive jpg: A progressive jpg (drastically simplifying, conflating features of a couple different versions (and maybe some speculative future tech)) consists of a header that gives a description of the shape of the image and some common information about how it should be decoded. Then the image is broken into a series of regions, ordered from larger to smaller, starting with the whole image at low detail, then splitting it into smaller regions with higher detail. During decoding, a higher detail region is layered on top of the lower detail region it refines (potentially adding to it to reduce the amount of baseline offset needed to encode in the child region and improving compression ratios). Therefore there is no causal dependency between sibling regions, only between parent and child regions. Transmitting this in a stream creates head of line blocking, where the reciever can't start decoding the top left region of the next layer until the bottom right region of the previous layer is fully recieved, and if any single packet is dropped the entire image stops refining until a whole retransmission round trip happens, giving a worse experience and higher latencies. If the underlying connection instead provided a ... Causal DAG .. of some kind where the jpeg encoding knowledge could just mark the exact dependencies between regions and the reciever could start decoding without waiting for siblings to complete, this would be a significant improvement for even ordinary web browsing, but beyond that something with nanite-style virtualized geometry and textures that supports any user generated content that needs to be streamed would be utterly unusable without a system like this. Expressing these dependencies is relatively straightforward in the paradigm of Object Capability based protocols with promise pipelining, but nothing I'm aware of even tries to do this despite being a huge performance win, vitally important for future technological development, and really naturally coupled to the causal requirements of ocap systems. There is a fair bit of work to do here figuring out how to compact the causal dag into as few bits as possible for common usecases and make programming language libraries naturally and intuitively express the causality. There's a bit of prior art in programming language causality usability like ponylang, and I have a plan for how to implement this all myself, but I'd really like to see other people researching, experimenting, and playing with it since more eyes makes the odds of finding something I missed higher.

Head of line blocking also has a mirror problem from the lack of being able to state that one request should follow another. In the classic HTTP paradigm, it's expected to make one request per stream connection, which means that if you want to POST one thing that affects another endpoint then GET the second endpoint to see the final state, you can't just send both requests in one stream and rely on the server not processing the second stream until the first completes. You instead have to send the POST request, wait for the response, then send the GET request. This doubles the latency over being able to specify causal ordering of requests so that requests can be pipelined before all their dependencies are satisfied. Additionally the single-stream-multi-connection architecture encourages DDOSing your own servers by making the intended usage of the service involve every user making dozens of connections simultaneously opening dozens of file handles requiring the kernel to allocate dozens of buffers which largely store redundant copies of the client identifying itself to the server. It should be easier to tell your own software and DDOS/slow loris attacks apart!

The model I'm proposing can be built on top of UDP, be fully encrypted with 0rtt initialization, and supports all the above usecases with low overhead. We can specify a causal dag, where each message can specify a set of dependencies, which it may pull data from or just have a causal dependency on. The sender of a message may refer to any message it wants to send as soon as it picks a message ID for it. The receiver of a message will wait until the dependencies of a message are satisfied to process it. This allows pipelining arbitrarily many messages in sequence and running arbitrary graphs of messages in optimal parallelism, while minimizing overhead. The exact layout of the format to take as few bits as possible on a message, and what all a message can be, can be discussed later.

We can define the semantics of the model as a directed acyclic graph of dependencies. Without getting too deep into how these properties are established, Each machine involved in a network has a local view of the network causality with regards to the DAG of messages, and no machine will ever observe the causality being violated. If two machines are both implementing the protocol correctly, then the pair of them together will never observe causality being violated even if they are both talking to a malicious actor and making causality constraints with respect to the third party; with a cryptographic integrity assumption this holds even if the malicious party is allowed to relay, reorder, drop, and duplicate packets. (at least up to "the connection will drop safely before any causality violating message is processed by the application" so an attacker can't force a semantic violation only strategically permit valid progress and cause a total failure in a manner the application can detect and react to.) The semantics are defined by "happens before" edges between messages, and can be expressed whether or not the messages have been sent yet. The "real" order can be defined as any linearization which preserves the DAG, which permits parallel processing, and the elision of constraints that it's no longer possible to violate to reduce packet sizes.

Of course we must assume that it is possible for a user to obtain a non-compromised computer with un-backdoored cryptographic primitives in order to progress with those security properties; justifying and establishing these assumptions is left as an exercise to the reader, sorry.


## Reliability and Unreliability

In the previous section about head of line blocking, we've talked about Sequencing and Unsequencing network requests. In classic networking protocols, you must pick between these; TCP gives you sequenced stream and UDP gives you unsequenced datagrams. But there's a second dimension here; TCP is a *reliable* sequenced stream and UDP provides *unreliable* datagrams. There are two more corners of that matrix which are necessary for a practical protocol, and I intend to not only cover those corners and why they're necessary, but provide a unifying model for everything in between like I did in the previous section.

In addition to reliable sequencing, there can be unreliable sequencing, where an earlier packet may never arrive after a later packet, but rather than the later packet being held until all earlier packets arrive, the later packet is delivered immediately and any earlier packets are then dropped when they arrive. This ensures that the latest up to date information is delivered as soon as it becomes available, and the application doesn't need to worry about recieving updates out of order. This is an excellent fit for things like progress notifications, where we don't care about hearing that it reached 68% complete if we already heard that it reached 70% complete. It can also fit very well for video game physics updates, since once we know the positions and velocities (and hitpoints and status effects) of everything in the scene, the client doesn't need the previous version anymore and can instead interpolate directly to the latest version, reducing lag even when under network load. Voice calls may also be suitable for this; dropping out of order sound chunks instead of playing them out of order may give a better experience, though tuning it to see how large the chunks should be and how much latency is beneficial to introduce would be necessary.

Reliable unsequencing can express the example of the progressive jpeg from earlier, where we don't care in which order the regions arrive as long as they all do. The additional power of the sequencing edges that ensure child regions arrive after parent edges from the model I proposed earlier, however, can slightly simplify the code to handle the image, and also improves the performance by allowing the retransmission logic to know which packets are most important to retransmit first for the best UX, while providing a good abstraction for providing that information at the application layer without needing to manually tear down the abstraction stack to mess with packets directly. Reliable unsequencing in general is the most efficient way to download files, which is why bittorrent implements it in a slightly hacky way, allowing the chunks of the file or collection of files to arrive in any order.

As we can see, all four quadrants of the matrix are vitally important for some purposes, and most applications (a chat program that can send both text messages and images, web browsers that load pages that have assets and call server endpoints, video games that can load assets while sharing physics updates and having in game voice chat) benefit from using multiple quandrants, including the nonstandard two I just described, and indeed would be most naturally expressed by switching back and forth between them within the scope of a single connection, also known as quadrant vaccilation. (That's a joke, it isn't actually called that, I just included that to jumpscare people with a specific internet background) The `enet` library for networking in games successfully implements all four quadrants on top of a UDP carrier, but the fixed channel layout and lack of encryption makes it unsuitable for more general usage.

However, you may have noticed a slight missing case in that previous model; in the unreliable sequenced case, it's frequently useful to retransmit the latest state to ensure the latest state arrives eventually if it isn't superceeded. For example in the progress notifications model, you do actually want to hear the very latest notification eventually even if it were lost initially and needed to be retransmitted. This is especially clear on the 100% state; it's desirable to actually know the process is finished. But even when there isn't a specific known stopping point it's still useful to learn the latest state eventually, even when future updates are still coming. For example, in any kind of collaborative editing system like r/place (to sidestep the complexities of crdts for a more flexible data layout, which still works but would require a detour into the antimatter history pruning crdt algorithms to explain how to make losing intermediate updates safe in a distributed system so people with moderate knowledge of the state of the art in document synchronization don't get mad) losing intermediate states that were never reflected on the client is fine, but as a user it would be highly preferable if a pixel didn't stay the wrong color on your screen until someone else drew on it.

To express these semantics, we can add the concept of a "supercession" relation (or rather variant of the happens before relation) into the causal DAG. This expands the set of possible linearizations to include those where a subset of messages which are only transitively accessible via supercession edges from messages which are included are omitted. It continues to exclude any linearization where a superceeded message comes after the successor.