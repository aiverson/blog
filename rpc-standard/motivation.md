
Networking is not very good or easy right now and no project I've been able to find could actually fix it. Problems include:
- Head-of-line blocking
- Lack of schematization with migratability and translatability and a useful type system
- Poor compression
- Poor security
- Poor language support
- Lack of promise pipelining
These are related problems, and solving one will frequently make others easier. 



## Security

Capabilities are the best option for modern security. They are intuitive, easy to statically analyze, and have fewer footguns than alternatives. Fortunately, it's also pretty easy to implement and scales very well to novel APIs, with a set of well researched constructions that can replicate the function of any other security system. Other people have already written a lot about this, so I won't go too in depth. If people want to know more, I'll post a followup about my own novel capsec throughts and/or links to prior art.

This speculative protocol is largely built around ocaps due to the flexibility and security of that architecture, but it's not essentially tied to the distinguished subject of a typical ocap system deriving from Java's OOP ontology. It could support non-object capabilities, or ocaps with multimethods if desired. The improved facilities from such features are interesting, but also straightforward to add as an extension after initial implementation if desired or to emulate with no additional primitives. It is currently unclear to me exactly what security implications such extensions would have or how to mitigate them. It could potentially make it easier to accidentally introduce unprivileged operations, but it could also make it easier to intentionally introduce them and to manage the combinatorial explosion of interoperability and flexible software without accidentally introducing security mistakes. To what extent these potential security vulnerabilities and benefits would actually come through in practice and to what extent they could be systematically mitigated and how would need further research, and we can still do much better than the existing state of the art without completing that.

## Promise Pipelining

Promise pipelining is one of the core features that makes all the other latency reduction techniques work; promise pipelining is a technology that allows you to refer to the result of a request before it returns as part of the parameters of another request. With promise pipelining, every request you send is given an ID as you send it that the other side of the connection can understand, and requests can refer to that ID as part of their payloads. To be practical, this requires the network system come with some kind of object model to be able to express what it means to refer to the response of a request, because sending over parsing logic to extract things from a serialized representation is... problematic. Whatever language is available to express how to do this *must* be sub-turing-complete so that it isn't possible to do long running or hostile tasks with it, it must be an extremely compact serialized bytecode that can get lost in the corner of a single UDP packet for simple operations, it must be very carefully locked down to avoid security problems by construction (processor vendors, please stop selling us broken hardware, it would make my job a lot easier if I could formally verify security against (a formalized version of) what you say your hardware does and just verify the hardware does as specified instead of needing to invent my own much messier formalism capturing a superset of what it actually does and hoping I'm correct. Please.), it must *not* have a common source code representation that people are expected to use but instead be synthesized directly by the networking library from promise manipulations done by the host language, it must be easy to generate so that it's practical to demand every language library implement a synthesizer, it must be able to be turned off cleanly without affecting the functionality or security of the protocol (only the performance, just in case processor vendors make another catastrophic fucky wucky and then don't fix it for years. again.), it must be impossible to make any part of the checking or evaluation of the reference take longer than linear time, and it must be possible to partially evaluate when some of the pending requests complete. Some of that merits expansion, so in no particular order:

Promise pipelining is a critical performance and usability feature for general purpose network protocols, because promise pipelining is what enables the single responsibility principle. Round trips are really really expensive compared to compute, and usually even compared to bandwidth. I encourage you to try to calculate what the break-even point is for sending, say 20% more data (a generous estimate) versus taking one additional full round trip is with respect to application responsiveness for your favorite server and device. How much data do you need to be sending constantly to saturate the connection enough that you'd rather do the latter than the former. And then consider that promise pipelining will usually *save* you data by allowing you to entirely avoid transmitting the intermediate results both ways. Without promise pipelining, to realize this performance benefit it is necessary to contort the API design around it, rather than having a single method/endpoint for each logical operation, making a single endpoint for each chain of operations that's commonly used, so instead of "search for thing" then "display details of thing" you'd get a single "search and display details" for each combination of ways things can be searched or displayed, producing a combinatorial explosion of handling, especially once you start chaining more things, for example "show the details of the thing's friend" or "count how many friends the thing has". This lack of compositionality means that backend servers frequently have to add new endpoints for every single new feature the frontend wants rather than just providing the set of operations the frontend may perform and letting the frontend compose them as appropriate. GraphQL is an example of this kind of aggregation in more conventional web technologies, where the backend can just express the set of operations available to the frontend and the common infrastructure provides a framework for composing them without extra round trips. GraphQL is of course very wasteful, using JSON and textual inputs for everything, and has a very sharp client/server distinction which limits the usefulness for more flexible architectures.

Regarding the linearity of checking and evaluation, we could instead have a VM that counts instruction costs to either limit them or charge them to an account in a variety of ways, for example by requiring the other side of the connection to present partial hash inversions to prove they spent at least as much effort preparing to ask you to run the code as they're expecting you to expend on their behalf, or by requiring every user to sign in to an accounting system that can charge tiny fractions of money for every request sent, etc. However, it is my opinion that requiring global internet finance integration for every single service that runs the infrastructure would be awful for unbanked, marginalized populations, and hobbyists who just want to play with small services, and therefore we shouldn't do it. Proof of work is also wasteful, and we should look harder for alternatives rather than baking that into more parts of our networking primitives; as anubis has shown, it might be valuable to require some proof of work as an anti-hostile-crawler/ddos measure, but I don't really want to get into that right now. More importantly, gas based VMs with more complicated control flow means increased vulnerability to side channel attacks and ability to run attack code and other hazards, so restricting to linear runtime serves dual purposes of both reducing attack surface and requiring the originator to have paid for the network transmission for every instruction they want you to execute. Calling methods can then be metered or billed if and as the host wants at the method level. Proving resistence to specter in a general VM is really hard actually, requiring things like working out exactly what it takes to separate a bounds check from an access well enough to ensure that cache misses, pipeline stalls, instruction reordering, and speculation can't bring them back together enough to form a complete exploit mechanism. Designing a mechanism that has no ability to make a scratch array, no ability to branch backwards to loop, no ability to write at variable offsets, gets a different randomly positioned stack space every time it is evaluated, and costs the attacker more money than the reciever makes it a lot easier. Since we live in a world where specter exists, and hasn't been fixed after years but only partially mitigated, along side vulnerabilities in constant time cryptography instructions, and so many others, making the mechanism complicated to secure is making it broken and insecure.

One of the possible objections to such a restricted mechanism is the way it prevents running things like filter/map/reduce queries locally with the data it is querying over, but while that is a valuable use case, this specific mechanism is a poor fit for it. Promise pipelining like this is inherently an imperative kind of action, "run this method, then run this method, then run this method" which means that if it is being used to perform queries directly, then it is forced to do full sequential loops across the whole dataset, or requires it to have been given access to all the relevant indices as object references and manually plan the query with respect to them which complicates the security dramatically. Or the promise pipelining VM language could have added features until it includes an entire subsystem for expressing relational queries and whatever other kinds of queries people want to use, but remember that the goal is to minimize the attack surface and runtime cost of a security and performance critical subsystem rather than multiply it exponentially. We can implement a querying subsystem separately anyways without baking it into the protocol, as a query object that can be referenced and requested and secured however is desired, that has a method that takes query descriptions and can plan and execute the queries with the more complicated query semantics, and can use the promise pipelining system to chain inputs and outputs between queries and other requests in a uniform manner, which allows the query system to evolve separately from the base protocol. Perhaps a successor protocol designed when we have much better computer science knowledge regarding compositional formalisms of queries and databases than we do now, cheaper computation than we do now, and better security and secure hardware than we do now could revisit this decision.

A possible pitfall of designing and implementing a promise pipelining system badly is making it distinguish between "pointers" and "primitives" so that the promise pipelining system can only operate on "pointers" by doing something like making references to previous requests or objects be implemented by something that's either a reference or a pointer indirection, essentially creating a linked list of much more expensive pointer offset ops rather than using a more compact unified model of message patching. Don't do that. Doing that forces interface awkwardness every time pipelining on a primitive is desired, requiring making every primitive in any interface that could want to have a primitive pipelined to be an enum between the primitive and an interface that has a single method to retrieve the primitive. This adds a ton of noise to using the interface since it now needs a bunch of enum wrappers, breaks separation of concerns by requiring the interface authors to consider and write workarounds for ways the interface can be pipelined, and inflates the size of the messages on the wire with all the additional enum tags and pointer references. Just choose not to make the obvious mistake.

## Sequencing and Unsequencing

Head of line blocking has been partially solved a couple times but people keep inventing it again and again every time, and I'm getting really frustrated with it. Head of line blocking can be regarded as spurious causal dependencies. Head of line blocking is a symptom of stream oriented systems (though not necessarily exclusive to them), where everything has a fully linear global order, and if one thing is slower everything "behind" it gets slowed down, even if there's no reason for it to depend on the previous packet. This can occur at the packet level, the content level, the request level, the application level, etc. in a bunch of ways. At the lowest level, when talking over a stream based protocol like TCP, if one packet gets dropped by congestion or intermittent link failure or anything, any subsequent packets have to wait for it to be retransmitted before they can be delivered. As an example of this, if you, for example, stream a video over a stream, and part way through realize that the network doesn't support the video's bitrate in realtime, if you send a request for a lower bitrate version the server can't start sending a new version of the video until you've recieved everything it "already" sent even if it never arrived, at a much lower rate than realtime forcing you to wait while a quality of video you don't want to watch anymore stuttery and slowly fills in before you start seeing the quality you can actually watch. To bypass this, HTTP based video serving systems spent a whole bunch of effort inventing a multiplexing protocol on top of the stream that establishes multiple streams and then sends different parts along different streams so that when they request a new lower quality version streams that are delivering the high quality version can be immediately killed without blocking the new version, etc. This is a complicated and inefficient solution that has spent many engineering hours adapting a technology never meant for this to support it.

Consider also a progressive jpg: A progressive jpg (drastically simplifying, conflating features of a couple different versions (and maybe some speculative future tech)) consists of a header that gives a description of the shape of the image and some common information about how it should be decoded. Then the image is broken into a series of regions, ordered from larger to smaller, starting with the whole image at low detail, then splitting it into smaller regions with higher detail. During decoding, a higher detail region is layered on top of the lower detail region it refines (potentially adding to it to reduce the amount of baseline offset needed to encode in the child region and improving compression ratios). Therefore there is no causal dependency between sibling regions, only between parent and child regions. Transmitting this in a stream creates head of line blocking, where the reciever can't start decoding the top left region of the next layer until the bottom right region of the previous layer is fully recieved, and if any single packet is dropped the entire image stops refining until a whole retransmission round trip happens, giving a worse experience and higher latencies. If the underlying connection instead provided a ... Causal DAG .. of some kind where the jpeg encoding knowledge could just mark the exact dependencies between regions and the reciever could start decoding without waiting for siblings to complete, this would be a significant improvement for even ordinary web browsing, but beyond that something with nanite-style virtualized geometry and textures that supports any user generated content that needs to be streamed would be utterly unusable without a system like this. Expressing these dependencies is relatively straightforward in the paradigm of Object Capability based protocols with promise pipelining, but nothing I'm aware of even tries to do this despite being a huge performance win, vitally important for future technological development, and really naturally coupled to the causal requirements of ocap systems. There is a fair bit of work to do here figuring out how to compact the causal dag into as few bits as possible for common usecases and make programming language libraries naturally and intuitively express the causality. There's a bit of prior art in programming language causality usability like ponylang, and I have a plan for how to implement this all myself, but I'd really like to see other people researching, experimenting, and playing with it since more eyes makes the odds of finding something I missed higher.

Head of line blocking also has a mirror problem from the lack of being able to state that one request should follow another. In the classic HTTP paradigm, it's expected to make one request per stream connection, which means that if you want to POST one thing that affects another endpoint then GET the second endpoint to see the final state, you can't just send each request in one stream and rely on the server not processing the second stream until the first completes. You instead have to send the POST request, wait for the response, then send the GET request. This doubles the latency over being able to specify causal ordering of requests so that requests can be pipelined before all their dependencies are satisfied. Additionally the single-stream-multi-connection architecture encourages DDOSing your own servers by making the intended usage of the service involve every user making dozens of connections simultaneously opening dozens of file handles requiring the kernel to allocate dozens of buffers which largely store redundant copies of the client identifying itself to the server. It should be easier to tell your own software and DDOS/slow loris attacks apart!

More modern HTTP supports sending multiple requests in a streamwhich allows opening one stream and sending both requests down it causing them to be processed in sequence. This removes a round trip for the case of a single linear causal dependency with no data dependency. Stream opening is also cheaper now so sending multiple requests is cheaper. But it falls short when a request needs to follow two requests that may be in parallel or when two parallel requests both need to follow a single request. It also cannot express referring to the result of a previous request without a full round trip.

Imagine if when requesting an HTTP document you could specify "$foo = GET /index/foo with etag FOO" then "GET $foo.stylesheet with etag FOOSTYLE" and "GET $foo.favicon with etag FOOICON" without needing to wait to parse the html in /index/foo. Webpages could load much faster!

The model I'm proposing can be built on top of UDP, be fully encrypted with 0rtt initialization, and supports all the above usecases with low overhead. We can specify a causal dag, where each message can specify a set of dependencies, which it may pull data from or just have a causal dependency on. The sender of a message may refer to any message it wants to send as soon as it picks a message ID for it. The receiver of a message will wait until the dependencies of a message are satisfied to process it. This allows pipelining arbitrarily many messages in sequence and running arbitrary graphs of messages in optimal parallelism, while minimizing overhead. The exact layout of the format to take as few bits as possible on a message, and what all a message can be, can be discussed later.

We can define the semantics of the model as a directed acyclic graph of dependencies. Without getting too deep into how these properties are established, Each machine involved in a network has a local view of the network causality with regards to the DAG of messages, and no machine will ever observe the causality being violated. If two machines are both implementing the protocol correctly, then the pair of them together will never observe causality being violated even if they are both talking to a malicious actor and making causality constraints with respect to the third party; with a cryptographic integrity assumption this holds even if the malicious party is allowed to relay, reorder, drop, and duplicate packets. (at least up to "the connection will drop safely before any causality violating message is accepted by the application" so an attacker can't force a semantic violation, only strategically permit valid progress and cause a total failure in a manner the application can detect and react to.) The semantics are defined by "happens before" edges between messages, and can be expressed whether or not the messages have been sent yet. The "real" order can be defined as any linearization which preserves the DAG, which permits parallel processing, and the elision of constraints that it's no longer possible to violate to reduce packet sizes.

Of course we must assume that it is possible for a user to obtain a non-compromised computer with un-backdoored cryptographic primitives in order to progress with those security properties; justifying and establishing these assumptions is left as an exercise to the reader, sorry.


## Reliability and Unreliability

In the previous section about head of line blocking, we've talked about Sequencing and Unsequencing network requests. In classic networking protocols, you must pick between these; TCP gives you sequenced stream and UDP gives you unsequenced datagrams. But there's a second dimension here; TCP is a *reliable* sequenced stream and UDP provides *unreliable* datagrams. There are two more corners of that matrix which are necessary for a practical protocol, and I intend to not only cover those corners and why they're necessary, but provide a unifying model for everything in between like I did in the previous section.

In addition to reliable sequencing, there can be unreliable sequencing, where an earlier packet may never arrive after a later packet, but rather than the later packet being held until all earlier packets arrive, the later packet is delivered immediately and any earlier packets are then dropped when (if) they arrive. This ensures that the latest up to date information is delivered as soon as it becomes available, and the application doesn't need to worry about recieving updates out of order. This is an excellent fit for things like progress notifications, where we don't care about hearing that it reached 68% complete if we already heard that it reached 70% complete. It can also fit very well for video game physics updates, since once we know the positions and velocities (and hitpoints and status effects etc.) of everything in the scene, the client doesn't need the previous version anymore and can instead interpolate directly to the latest version, reducing lag even when under network load. Voice calls may also be suitable for this; dropping out of order sound chunks instead of playing them out of order may give a better experience, though tuning it to see how large the chunks should be and how much latency is beneficial to introduce would be necessary.

Reliable unsequencing can express the example of the progressive jpeg from earlier, where we don't care in which order the regions arrive as long as they all do. The additional power of the sequencing edges that ensure child regions arrive after parent edges from the model I proposed earlier, however, can slightly simplify the code to handle the image, and also improves the performance by allowing the retransmission logic to know which packets are most important to retransmit first for the best UX, while providing a good abstraction for providing that information at the application layer without needing to manually tear down the abstraction stack to mess with packets directly. Reliable unsequencing in general is the most efficient way to download files, which is why bittorrent implements it in a slightly hacky way, allowing the chunks of the file or collection of files to arrive in any order.

As we can see, all four quadrants of the matrix are vitally important for some purposes, and most applications (a chat program that can send both text messages and images, web browsers that load pages that have assets and call server endpoints, video games that can load assets while sharing physics updates and having in game voice chat) benefit from using multiple quandrants, including the nonstandard two I just described, and indeed would be most naturally expressed by switching back and forth between them within the scope of a single connection, also known as quadrant vaccilation. (That's a joke, it isn't actually called that, I just included that to jumpscare people with a specific internet background) The `enet` library for networking in games successfully implements three of the four quadrants on top of a UDP carrier, but the fixed channel layout and lack of encryption makes it unsuitable for more general usage. Quic successfully implements two (and a half?) but is badly architected in a way that makes it difficult and inefficient to extend to all four quadrants, however it does have excellent support for encryption and traffic shaping.

However, you may have noticed a slight missing case in that previous model; in the unreliable sequenced case, it's frequently useful to retransmit the latest state to ensure the latest state arrives eventually if it isn't superceeded. For example in the progress notifications model, you do actually want to hear the very latest notification eventually even if it were lost initially and needed to be retransmitted. This is especially clear on the 100% state; it's desirable to actually know the process is finished. But even when there isn't a specific known stopping point it's still useful to learn the latest state eventually, even when future updates are still coming. For example, in any kind of collaborative editing system like r/place (to sidestep the complexities of crdts for a more flexible data layout, which still works but would require a detour into the antimatter history pruning crdt algorithms to explain how to make losing intermediate updates safe in a distributed system so people with moderate knowledge of the state of the art in document synchronization don't get mad) losing intermediate states that were never reflected on the client is fine, but as a user it would be highly preferable if a pixel didn't stay the wrong color on your screen until someone else drew on it again.

To express these semantics, we can add the concept of a "supercession" relation (or rather variant of the happens before relation) into the causal DAG. This expands the set of possible linearizations to include those where a subset of messages which are only transitively accessible via supercession edges from messages which are included are omitted. It continues to exclude any linearization where a superceeded message comes after the successor.

So, to summarize: sometimes we want information in exactly a specific order, sometimes we just want all the information to arrive but it's useful in any order, sometimes we want the latest information and older information is useless, sometimes (rarely) we want information immediately or it's useless. And most frequently of all, we have multiple categories combining with irregular dependencies on each other. The protocol must be able to provide all those semantics and avoid retransmitting useless information and prioritize sending information that's most needed to achieve the point of optimal performance on the bandwidth/latency curve.

### Fault Tolerance

When handling reliable or semi-reliable transport, it is important to address the fault tolerance properties. As we know from the byzantine fault tolerance theorems, it's not possible to get perfect notification on both sides in all cases, so instead we should be clear about exactly where the fault tolerance guarantee lies. For these purposes, the fault tolerance guarantee is that no sequenced operation will happen while one of its prerequisites didn't regardless of the network conditions and that on a graceful termination, the sender will know that everything it sent that is reliably constrained to happen before the end of the connection will have happened, though it isn't possible to know that the other side of the connection also knows that everything the other side sent also arrived. Heartbeat packets however can maintain the regress of "I know that you know that I know" while the connection is otherwise idle, and the causal constraints will establish that earlier messages are mutually shared, though most applications won't care about the exact level of assurance on each message and instead just rely on the causal coherence and safe termination to get it good enough for their purposes.

Transactions would be a useful enhancement as well, to guarantee that of a batch of messages, the state and causality reflect either all of the messages or none of the messages, even in the presence of connection failures. For single party transactions it's relatively straightforward, just adding a mechanism to the connection to create a transaction, then attach messages to the transaction, then complete the transaction, which could have no overhead when not in use because it can be fit inside an existing enum field that's already required in the protocol. Multiparty transactions, however, which could integrate multiple peers in a transaction, allowing callbacks to another machine and other multi-party operations, is much more complicated and it's not clear to me how to implement these in an easily usable mechanism which can be widely adopted across multiple interacting systems. I know it's possible to implement this kind of technique because existing distributed database engines have this feature, but the documentation doesn't go into detail into how to implement them. It would probably need significant runtime support to make it low effort on application writers. If anyone happens to have the required specialist knowledge to talk about how to build this sort of thing, please get in touch, I'd love to work together.

## Robustness, Schematization, Versioning, and Localization

A networked protocol intended to be widely used across the internet must be designed to work across multiple languages, both computer and human, including ones that haven't been invented yet. A network spans many people, and must have consideration for communicating not merely the data the protocol is intended to transfer, but also how to use the protocol. A network spans time, and must have consideration for preserving institutional knowledge and onboarding new developers. This means that every single programmer interacting with the protocol libraries and services over the protocol must be provided with high quality inline documentation, autocomplete, and type information for absolutely everything directly in the IDE in the computer language they're writing and their native human language if possible or a common human language if not. There will be closed source services, or services where the creator wrote illegible code, or services where the code or comments have vanished beyond archaeological recovery. The protocol must remain stable over the course of decades through maintenance and extension. Old code must continue to work and remain secure for much longer than anyone expected it to, because someone's life will depend on a decade old interface to some random device working at a critical moment at some point, and it won't even have the good grace to be an obvious medical or safety system with a manufacturer still in business. The institutional knowledge of how to use any single service and the protocol as a whole must be preserved, and therefore must have a single source of truth which can be machine-checked for compatibility and is amenable to human translation and replication. But maintaining this must not produce a quadratically increasing burden as every new system needs to support the quirks of every existing system. We must always remember [Maintaining Robust Protocols](https://www.rfc-editor.org/rfc/rfc9413.html)

To accomplish these things, it is essential that identifying the interface to call a method on, and which method we want to call from that interface be non-human-relevant IDs, and also that interfaces must actually be part of the protocol.

### Principled Extensibility, something about errors, please improve this title



### Backpressure and Prioritization

## Multiparty features

If networking were always only between two peers, it would be pointless because you could just pick the two peers up and bring them to the same room. To actually be valuable, networking must have support for connecting to multiple peers. In fact, without any multiparty features the only option to establish networking is to have gone to the same room, and run cables. Therefore, the ability to work with multiparty features is a major component of networking protocols being usable. At some point, someone will want to return a reference to an object hosted on another machine from a request to a third, and then we have to deal with it.

One of the fundamental pieces of modern multiparty support is URL/URI/IRI features, which creates an international standard for encoding in text an unambiguous, machine-parseable reference to a specific thing or instruction to connect to a specific peer and ask it for a specific thing and even works across protocols without pairwise coordination. It has a few shortcomings of course, but it's such a good standard, so well architected, and so widely useful that it is mandatory for basically everything going forward, and if it ever dies it will be after a dramatic change to the way that data and documents are stored in general and then many decades of everything on the internet slowly being ported to the new formats until the last printed QR code containing a URL is destroyed. I don't actually think it's meaningfully possible to improve the standard for that at this point, it's just so well optimized for its purpose and medium. That said, what don't they capture well? They don't embed the cryptographic certificates for 0rtt connection establishment, they can't do promise pipelining, they don't inherently express fallback behavior to machines, and they are tied to a notion of a centralized global namespace. The standards for URLs make the tradeoffs they do for a reason, and can't really be improved within their constraints. For URLs the fallback behavior is embedded in DNS, which is a separate protocol required for it to work, which expresses how to actually connect to a specifc domain within the URL, at least for the schemes which use domains rather than some other system. The usage of a centralized global namespace is essential to be able to write down or type in URLs when moving things in or out of computers. DNS implements such a centralized global namespace very effectively and robustly. Cryptographic certificates are large, and would be impractical to attach to URLs for ordinary network connections. Being able to type a URL from memory or note is useful in a world where we don't all have implanted computers in our brains (or at least a really well designed personal network and services distributed across personal devices) capable of remembering and transmitting and authenticating cryptographic keys for us. We don't have that, nor are we likely to by next year. And promise pipelining would require cooperation of both the protocol the URL is being transmitted on and the protocol the URL points to, which is impossible for URLs to ensure. Some URL schemes do embed some of that information inside them, but it's protocol specific, and does get kinda unwieldy, so it's understandable to not be the main public-facing mechanism for the common ones. Therefore, we do need to maintain IRI compatibility for going from outside the protocol to inside, and be able to produce IRIs to hand things out outside the protocol, but maybe we can implement something inside the protocol that can do better than IRIs. 

Some of the assumptions of URI design can be discarded for alternative assumptions based on the protocol. We don't need to be able to write them in text, they can be structured data. We don't need to have them be human readable because they're internal machine-to-machine handoffs, so they can contain more complicated data. They can assume that everything agrees on pipelining aware ocap based protocols rather than being transport agnostic.

By providing protocol support for such references, we can enable better least-privilege security, where instead of a particular tool requiring the permission "arbitrary network access" and then using a URL to decide what to connect to (or connecting to anything else it feels like), the library or tool can instead be given a reference directly to a specific object on a specific connection and only be able to talk to that object and things the object says are allowed. If we can make this at least as easy as providing a URL for things with ubiquitous language support, we can make spyware, ransomware, and bugs with a blast radius much rarer.

### The Three Way Handoff

If Alice, Bob, and Charlie are in active communication and Alice has a reference to something of Bob's that she wants to send to Charlie, she can message both simultaneously, informing Bob that the thing is being sent and Charlie where to get the thing, which Bob and Charlie can then use to connect and figure out how to share the reference. This can be accomplished by having a shared unique nonce to perform the exchange, and including connection information for both. If this connection information includes both network address and cryptographic information, then both parties can perform a UDP holepunch (if necessary) and zero-round-trip connection startup at the same time. This could respect any current form of proxy or load balancer and even be performed over a UDP capable Tor successor, for example a geospatial-aware multicast-capable encrypted routing system which I theorize is possible but haven't actually attempted to specify or implement yet. It's also possible to include a fallback order so that the devices can try multiple connection methods in case some are blocked, and make it modular for forms of addressing that don't yet exist.

This three-way handoff can be promise pipelined across, either forwarding via Alice or connecting directly, using an embargo mechanism to enforce a causality relationship between pipelined requests from before the share and the requests sent along the newly established connection. This mechanism requires a little bit more sophistication than present in prior art to support full causal DAG integration, so that some messages along the new connection can be processed in parallel with still-being-delivered forwarded messages when causality permits. The exact bit layout of such a mechanism is beyond the scope of this document.

### The Four Way Join

If Alfred, Barbara, Cass, and Damian are trying to agree on something. In particular, Barbara and Cass both have a reference to something they claim is from Damian, but they're tricky and might have made a reference to something else instead to pass off as Damian's. If Alfred hasn't talked to Damian in a while, Alfred might not have the very latest keys for Damian, since they're all well aware of the Cryptographic Doom principle and rotate keys and/or algorithms regularly. So how is Alfred to know which key is really Damian's and which isn't? We can use a multi-way join operator which sends a nonce and a section of connection information along each of the component reference of the join, so that when they reassemble at Damian they form a complete direct connection and cap share, and possibly sending a fragmented direct connection authorization back across the set of connections and build a full reference. We can use a standard multi-part m-of-n encryption scheme to ensure that there is no way for any intermediate hop to intercept the connection without causing a failure, and so that it doesn't disclose the other path's details. (unless the all paths lead through the same intermediate hop in which case that intermediate hop is *supposed* to have the authority to dictate what those caps connect to and is allowed to decide whether or not they're the same).

This is a remarkably flexible primitive for combining references and permissions; in addition to just deciding that two things are the same, it can also express almost any form of privilege composition where multiple partial privileges are required to fully authorize to something. For example, a read-only reference to a specific thing in the database could be joined with a permission to write to an enclosing table or the root to obtain a reference with write permissions. Many papers have been written about this topic, and pitfalls and race conditions are well documented with known workarounds; it has also been proven equipotent with several other capability building blocks.


### Una and Multicasting

An Unum (plural Una) is a kind of object like thing that is itself distributed; to avoid confusion with an actual Object in an OOP system they need another name. An Unum is made of a set of Presences which can communicate, each of which lives on a single specific system and may be implemented as an ordinary Object, but the Unum as a whole is a single conceptual entity. Some familiar examples of Una are a Google Doc, a Git repo, a DNS cache, and a peer to peer voice call. In a Google Doc, there is a local Object that responds to your keystrokes and tells your device what to render onscreen, but that object isn't The Google Doc; there is a remote Object on google's servers that responds to edit messages and forwards them across the network, but that object isn't The Google Doc; The Google Doc is all of the presence on your device, the presence on your friend's device who you gave edit permission, the presence on all the others who can only read it, and the presence on Google's coordination and storage server(s) that acts as a lighthouse so all the other presences can find each other. Similarly for a git repo, except instead of having a distinction between a server presence which connects to many clients (and handles server distribution and load balancing behind the scenes) and a client presence which only connects to the server, git repos instead have many remotes they all connect to intermittently and no distinction in presence roles. A DNS cache can answer requests itself or forward them onwards until reaching the set of root DNS servers that all share information among themselves, forming a sort of tree of presences that are all conceptually The Domain Name System. A voice call remains even as people join and leave and the call migrates between relays, until the call still exists and no piece of hardware that was originally part of it still is. But common to all of these systems, the actual *Thing* isn't solely in one specific place; there is a piece of local code that things which want to interact with the unum talk to, which responds to local methods and talks to other presences over the network when it needs to, as opposed to there being a single remote server that all operations need to talk to, like an SVN server.

At a basic level, this can be implemented entirely in existing object primitives on the protocol level regarding una solely as wrapper code that implement client-side layers around specific caps. To improve usability at a language level, we can integrate a mechanism into the language library and/or codegen to have the serialization code automatically interoperate with una client libraries for certain types, allowing applications that use a common set of una to act like they have first class support. Some potentially valuable una would be things like the schemas themselves and generic types from the schema (structural equality on generic types regardless of who instantiated them without needing to serialize/transmit the whole structural tree at every reference) However, by adding una specific features to the protocol, we can potentially improve support further or reduce the overhead of common unum operations. 

Inspired by Soatok's [A plan for Multicast Support in Noise Based Protocols](https://soatok.blog/2023/10/10/a-plan-for-multicast-support-in-noise-based-protocols/), we can reduce the network bandwidth cost of multi-peer una. The common communication patterns for Una are "send to all peer presences I'm aware of", "send to all the peer presences I'm aware of except the one that I'm handling a message from", and "Send to a specific peer presence"; uncommonly, "send to a specified set of peers" might also be useful. Integrating multicast into the protocol so that packets only get duplicated as late across the network as possible can reduce the total network bandwidth of una messaging. The common unum communication patterns can be reified into the library and opportunistically use multicast to send messages to multiple presences. We can extend a three way handoff to support enrollement into a ratchet tree while maintaining causality to ensure correct sequencing and reliability.

Programming Una is necessarily difficult since it requires reasoning about a distributed system, considering the bandwidth cost of potentially quadraticly many messages, and providing a performant implementation usable ergonomically from every participating language. Programming Una is also necessary, because having local responses to low-latency operations and smarter remote sync protocols are so critical to building modern responsive, reliable, and robust infrastructure. So, doing things to reduce the overhead and enhance the baseline security of una is important. Encrypted multicast support gives better bandwidth utilization and saves lots of security headaches. A common distributed transaction mechanism makes getting una to agree on their state across the network much easier to set up. Perhaps later a wasm-like runtime with a safe interface optimized for talking across the protocol and interfaces can make packaging una code easier and provide a single least-privilege implementation that can be tested and fuzzed to tartarus and back.